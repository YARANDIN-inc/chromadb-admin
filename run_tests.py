#!/usr/bin/env python3
"""
Test runner script for ChromaDB Admin application.
Provides different test execution modes and configurations.
"""

import os
import sys
import subprocess
import argparse
from typing import List, Optional


def run_command(cmd: List[str], description: str) -> bool:
    """Run a command and return success status"""
    print(f"\n{'='*60}")
    print(f"Running: {description}")
    print(f"Command: {' '.join(cmd)}")
    print('='*60)
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=False)
        print(f"âœ… {description} completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} failed with exit code {e.returncode}")
        return False


def setup_test_environment():
    """Set up test environment variables"""
    os.environ.update({
        'TEST_MODE': '1',
        'SECRET_KEY': 'test-secret-key-for-testing-only',
        'DATABASE_URL': 'sqlite:///./test.db',
        'CREATE_INITIAL_ADMIN': 'false',
        'CHROMADB_HOST': 'localhost',
        'CHROMADB_PORT': '8000',
        'TARGET_PASS_RATE': '80.0'  # Set 80% pass rate by default
    })
    print("Test environment variables set")
    print(f"ğŸ¯ Target pass rate: {os.environ.get('TARGET_PASS_RATE')}%")


def run_unit_tests(verbose: bool = False, coverage: bool = True) -> bool:
    """Run unit tests"""
    cmd = ['python', '-m', 'pytest', 'tests/', '-m', 'not integration and not slow']
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    if coverage:
        cmd.extend(['--cov=app', '--cov-report=term-missing'])
    
    return run_command(cmd, "Unit Tests")


def run_integration_tests(verbose: bool = False) -> bool:
    """Run integration tests"""
    cmd = ['python', '-m', 'pytest', 'tests/', '-m', 'integration']
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    return run_command(cmd, "Integration Tests")


def run_all_tests(verbose: bool = False, coverage: bool = True) -> bool:
    """Run all tests"""
    cmd = ['python', '-m', 'pytest', 'tests/']
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    if coverage:
        cmd.extend(['--cov=app', '--cov-report=html', '--cov-report=term-missing'])
    
    return run_command(cmd, "All Tests")


def run_specific_test_file(test_file: str, verbose: bool = False) -> bool:
    """Run tests from a specific file"""
    cmd = ['python', '-m', 'pytest', f'tests/{test_file}']
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    return run_command(cmd, f"Tests from {test_file}")


def run_tests_by_marker(marker: str, verbose: bool = False) -> bool:
    """Run tests with specific marker"""
    cmd = ['python', '-m', 'pytest', 'tests/', '-m', marker]
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    return run_command(cmd, f"Tests with marker: {marker}")


def run_linting() -> bool:
    """Run code linting"""
    success = True
    
    # Check if flake8 is available
    try:
        subprocess.run(['flake8', '--version'], check=True, capture_output=True)
        success &= run_command(['flake8', 'app/', 'tests/'], "Flake8 Linting")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸  Flake8 not available, skipping linting")
    
    # Check if black is available
    try:
        subprocess.run(['black', '--version'], check=True, capture_output=True)
        success &= run_command(['black', '--check', '--diff', 'app/', 'tests/'], "Black Code Formatting Check")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸  Black not available, skipping format check")
    
    return success


def run_type_checking() -> bool:
    """Run type checking with mypy"""
    try:
        subprocess.run(['mypy', '--version'], check=True, capture_output=True)
        return run_command(['mypy', 'app/'], "MyPy Type Checking")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸  MyPy not available, skipping type checking")
        return True


def clean_test_artifacts():
    """Clean up test artifacts"""
    import shutil
    import glob
    
    artifacts = [
        'test.db',
        '.coverage',
        'htmlcov/',
        '.pytest_cache/',
        '**/__pycache__/',
        '**/*.pyc'
    ]
    
    for pattern in artifacts:
        for path in glob.glob(pattern, recursive=True):
            try:
                if os.path.isfile(path):
                    os.remove(path)
                elif os.path.isdir(path):
                    shutil.rmtree(path)
                print(f"Cleaned: {path}")
            except OSError:
                pass


def install_test_dependencies() -> bool:
    """Install test dependencies"""
    return run_command([
        'pip', 'install', '-r', 'requirements.txt'
    ], "Installing Test Dependencies")


def run_all_tests_with_pass_rate(verbose: bool = False, coverage: bool = True, target_pass_rate: float = 80.0) -> bool:
    """Run all tests with pass rate evaluation instead of strict success"""
    cmd = ['python', '-m', 'pytest', 'tests/']
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    if coverage:
        cmd.extend(['--cov=app', '--cov-report=html', '--cov-report=term-missing'])
    
    # Set target pass rate in environment
    os.environ['TARGET_PASS_RATE'] = str(target_pass_rate)
    
    print(f"\nğŸš€ Running all tests with {target_pass_rate}% pass rate tolerance")
    return run_command(cmd, f"All Tests (Target: {target_pass_rate}%)")


def run_unit_tests_with_pass_rate(verbose: bool = False, coverage: bool = True, target_pass_rate: float = 80.0) -> bool:
    """Run unit tests with pass rate evaluation"""
    cmd = ['python', '-m', 'pytest', 'tests/', '-m', 'not integration and not slow']
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    if coverage:
        cmd.extend(['--cov=app', '--cov-report=term-missing'])
    
    os.environ['TARGET_PASS_RATE'] = str(target_pass_rate)
    
    print(f"\nğŸ§ª Running unit tests with {target_pass_rate}% pass rate tolerance")
    return run_command(cmd, f"Unit Tests (Target: {target_pass_rate}%)")


def run_integration_tests_with_pass_rate(verbose: bool = False, target_pass_rate: float = 80.0) -> bool:
    """Run integration tests with pass rate evaluation"""
    cmd = ['python', '-m', 'pytest', 'tests/', '-m', 'integration']
    
    if verbose:
        cmd.extend(['-v', '-s'])
    
    os.environ['TARGET_PASS_RATE'] = str(target_pass_rate)
    
    print(f"\nğŸ”— Running integration tests with {target_pass_rate}% pass rate tolerance")
    return run_command(cmd, f"Integration Tests (Target: {target_pass_rate}%)")


def main():
    parser = argparse.ArgumentParser(description='ChromaDB Admin Test Runner')
    parser.add_argument('--mode', choices=['unit', 'integration', 'all', 'file', 'marker'], 
                       default='all', help='Test execution mode')
    parser.add_argument('--file', help='Specific test file to run (for file mode)')
    parser.add_argument('--marker', help='Test marker to filter by (for marker mode)')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--no-coverage', action='store_true', help='Skip coverage reporting')
    parser.add_argument('--lint', action='store_true', help='Run linting')
    parser.add_argument('--type-check', action='store_true', help='Run type checking')
    parser.add_argument('--install-deps', action='store_true', help='Install dependencies first')
    parser.add_argument('--clean', action='store_true', help='Clean test artifacts before running')
    parser.add_argument('--ci', action='store_true', help='CI mode (install deps, run all checks, no interactive)')
    
    # New pass rate management options
    parser.add_argument('--pass-rate', type=float, default=80.0, 
                       help='Target pass rate percentage (default: 80.0)')
    parser.add_argument('--strict', action='store_true', 
                       help='Strict mode: all tests must pass (overrides pass rate)')
    parser.add_argument('--save-results', metavar='FILE', 
                       help='Save test results to JSON file')
    
    args = parser.parse_args()
    
    # Setup
    setup_test_environment()
    
    # Set pass rate configuration
    if args.strict:
        target_pass_rate = 100.0
        print("ğŸ”’ Running in STRICT mode: all tests must pass")
    else:
        target_pass_rate = args.pass_rate
        print(f"ğŸ“Š Running with {target_pass_rate}% pass rate tolerance")
    
    if args.save_results:
        os.environ['TEST_RESULTS_FILE'] = args.save_results
        print(f"ğŸ’¾ Results will be saved to {args.save_results}")
    
    if args.clean:
        clean_test_artifacts()
    
    if args.install_deps or args.ci:
        if not install_test_dependencies():
            sys.exit(1)
    
    success = True
    
    # CI mode runs everything with pass rate tolerance
    if args.ci:
        print("ğŸš€ Running in CI mode - executing all checks with pass rate tolerance")
        success &= run_linting()
        success &= run_type_checking()
        success &= run_all_tests_with_pass_rate(verbose=True, coverage=True, target_pass_rate=target_pass_rate)
    else:
        # Run linting if requested
        if args.lint:
            success &= run_linting()
        
        # Run type checking if requested
        if args.type_check:
            success &= run_type_checking()
        
        # Run tests based on mode with pass rate tolerance
        coverage = not args.no_coverage
        
        if args.mode == 'unit':
            success &= run_unit_tests_with_pass_rate(args.verbose, coverage, target_pass_rate)
        elif args.mode == 'integration':
            success &= run_integration_tests_with_pass_rate(args.verbose, target_pass_rate)
        elif args.mode == 'all':
            success &= run_all_tests_with_pass_rate(args.verbose, coverage, target_pass_rate)
        elif args.mode == 'file':
            if not args.file:
                print("âŒ --file argument required for file mode")
                sys.exit(1)
            # For specific files, set pass rate and use regular runner
            os.environ['TARGET_PASS_RATE'] = str(target_pass_rate)
            success &= run_specific_test_file(args.file, args.verbose)
        elif args.mode == 'marker':
            if not args.marker:
                print("âŒ --marker argument required for marker mode")
                sys.exit(1)
            # For specific markers, set pass rate and use regular runner
            os.environ['TARGET_PASS_RATE'] = str(target_pass_rate)
            success &= run_tests_by_marker(args.marker, args.verbose)
    
    # Summary
    print("\n" + "="*60)
    if success:
        if args.strict:
            print("ğŸ‰ All tests and checks completed successfully!")
        else:
            print(f"ğŸ‰ Tests meet the {target_pass_rate}% pass rate requirement!")
        print("âœ… Build is ready for deployment")
    else:
        if args.strict:
            print("ğŸ’¥ Some tests or checks failed!")
        else:
            print(f"ğŸ’¥ Tests do not meet the {target_pass_rate}% pass rate requirement!")
        print("âŒ Please fix the issues before proceeding")
    print("="*60)
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main() 